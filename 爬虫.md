# 爬虫

爬虫：通过编写程序，模拟浏览器上网，然后让其互联网上抓取数据的过程；

价值：

- 
在实际应用上

- 
就业上

- 
时常优化自己的程序，避免干扰被访问网站的正常运行

- 
在使用，传播爬取到的数据时，要审查抓取到的内容涉及到用户隐私或者商业机密等敏感内容及时停止抓取


爬虫的分类：

- 
通用爬虫：抓取系统重要组成部分，抓取的是一整张页面数据

- 
聚焦爬虫：是建立在通用爬虫的基础之上，抓取的是页面中特定的局部内容

- 
增量式爬虫：检测网站中数据更新的情况。只会抓取网站中最新更新出来的数据


反爬机制：一些网站，可以通过制定相应的策略或者技术手段，防止爬虫程序进行数据的爬取

反反爬策略：爬虫程序可以通过制定相关的策略或者技术手段，破解门户网站中的反爬机制

## robots.txt 协议（君子协议）

君子协议。规定了网站哪些数据可以爬取，哪些不可以爬取。

查看网站的 robots.txt 协议内容：

[淘宝](https://www.taobao.com/robots.txt)

在网页后面加上/robots.txt是可以看到这个网页可以被爬取的数据。若是product则是不允许被爬取的

[bibi](https://www.bilibili.com/robots.txt)

[知乎](https://www.zhihu.com/robots.txt)

## http 协议

常用请求头信息：

- 
User-Agent:请求载体的身份标识

- 
Connection:请求完毕后，是断开连接还是保持连接


常用响应头信息

- Content-Type:服务器响应加客户端的数据类型

https 协议：

- s 表示安全的 http 协议

加密方式：

- 
对称秘钥加密

- 
非对称秘钥加密

- 
证书


### request模块

### urllib模块

request 模块：python 中原生的一个基于网络请求的模块，功能非常强大，简单便捷，效率极高

作用：模拟浏览器请求

### 使用：

步骤：(request模拟的编码流程)

1. 
指定 url

2. 
发起请求

3. 
获取响应数据

4. 
持久化存储（就是存储爬取的数据）


使用命令：pip install requests 下载模块

代码实现：

需求：爬取搜狗首先

```python
# 需求：爬取搜狗首页
import requests as req

# 第一步，指定 url
url = "https://www.sogou.com"

# 第二步，发起请求 ,get 方法会返回一个响应对象
response=req.get(url)
# 第三步，获取响应数据,text 返回的是字符串形式的响应数据
data=response.text
print(data)
# 持久化存储
with open('存储文件.txt','w',encoding="utf-8") as fp:
    fp.write(data)
```

# 五个案例

'''

1.简易网页采集器

2.破解百度翻译

3.爬取豆瓣电影分类排行榜

4.爬取肯德基餐厅

5.爬取国家药品监督局

'''

### 案例一

```python
# UA：User-Agent (请求载体的身份标识）
# UA 检测：门户网站的服务器会检测对应请求的载体身体标识，如果检测到请求的载体身份标识为
#某一款浏览器，说明这个请求是一个正常的请求，但是，如果检测到请求的载体身份标识不是基于
#某一款浏览器的，则表示这个请求为不正常的请求，则服务器可能会拒绝请求
#所以编写爬虫的时候需要进行 UA 伪装
#打开浏览器打开开发者工具网络里面以送一个请求就可以看到
#eg:User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.0 Safari/537.36 Edg/103.0.1264.2
# UA 伪装
import requests
#================1.=============================

#UA 伪装
headers={
    'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.0 Safari/537.36 Edg/103.0.1264.2'
}
#'https://www.sogou.com/web?query=沈扬'
url='https://www.sogou.com/web'
# 处理url携带的参数：封装到字典中
kw=input('输入一个关键词：')
param={ # 封装到字典中
    'query':kw
}
# param 就是 url 的参数, headers 用于 UA 伪装
#注意这里get 方法要传入三个参数要指定参数，不然会报错：只能传入两个参数却传入了三个
response = requests.get(url=url,params=param,headers=headers)
data=response.text
with open('存储文件.html',"w",encoding='utf-8') as fp:
    fp.write(data)
print(data)
```

**注意这里的 UA 伪装**

### 案例二

UA 检测对应 UA伪装

```python
import json
import  requests
# 抓包工具可以知道，百度翻译发起的请求是 POST 请求
# 响应的数据是 JSON 格式
post_url="https://fanyi.baidu.com/sug"
# post 请求参数处理（同 get 请求一样）
kw=input("请输入要翻译的单词：")
data={
    'kw':'dog'
}
# 进行UA 伪装
headers={
    'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.0 Safari/537.36 Edg/103.0.1264.2'
}
response=requests.post(url=post_url,data=data,headers=headers)
# 获取响应数据：JSON() 方法返回的是 obj，（返回的格式是 json 才可以使用这 json方法）
dic_obj=response.json()
with open('存储文件.html','w',encoding='utf-8') as fp:
    json.dump(dic_obj,fp=fp,ensure_ascii=False)
```

案例三：

```python
# 3.爬取豆瓣电影分类排行榜
import requests
import json
url='https://movie.douban.com/chart'
param = {
'type':'24',
'interval_id':'100:90',
'action':'',
'start':'1',  # 从库中的第几部开始取
'limit':'20'  # 一次取出的个数
}
# 进行UA 伪装
headers={
    'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.0 Safari/537.36 Edg/103.0.1264.2'
}
response=requests.get(url=url,headers=headers)
list_data=response.text
fp=open('存储文件.json','w',encoding='utf-8')
fp.write(list_data)
```

## 数据解析

聚焦爬虫：爬取页面中指定的页面内容。

数据解析分类：

- 
正则 

- 
bs4

- 
xpath （** *）

- 
进行指定标签的定位

- 
标签或者标签对应的属性中存储的数据值进行提取（解析）


编码流程：

- 
指定url

- 
发起请求

- 
获取响应数据

- 
数据解析

- 
持久化存储数据


多了数据解析

```python
import requests
url='https://cn.bing.com'
headers={
    'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.0 Safari/537.36 Edg/103.0.1264.2'
}
# text （字符串） content （二进制） json() （对象）
img_data=requests.get(url=url,headers=headers).content
with open('1.jpg', 'wb') as fp:
    fp.write(img_data)
```

也就是数据匹配，匹配 html 标签。

使用正则表达式匹配

# xpath 解析：

'''

最常用最便捷最高效的一种解析方式。也通用。

1. 
实例化一个 etree 的对象，并且需要将被解析的页面源码数据加载到这个对象中

2. 
调用etree 对象中的 xpath 方法结合着 xpath 表达式实现标签的定位和内容的捕获。


环境的安装：

`pip install lxml`

**实例化一个 etree 对象：**

- 
将本地的 html 文档中的源码数据加载到 etree 对象中：etree.parse(filePath)

- 
可以将从互联网上获取的源码数据加载这个对象中：etree.HTML('page_text')


xpath表达式：

`/` 表示是从根节点开始定位，表示的是一个层级

`//` 表示是多个层级，可以表示从任意位置开始定位

属性定位： //tag[@attrName="attrValue"] 

eg: //div[@class="sy"]/p[1] # 这里的索引是从 1 开始的

这个功能是：匹配html 下的 div 的类名为 sy 下的第一个 p 标签

### 取文本：

r=tr.xpath('/html//div/text()')[0]

/text()  获取的是标签中直系的文本内容

//text() 获取的是标签中非直系的文本内容（所有的文本内容）

### 取属性：

/[@attrName ](/attrName )  => img/src 

取img 标签下src 的属性值

```python
# 首先需要导包
import requests
from lxml import etree
# 实例好了个 etree 对象，且将被解析的源码加载到对象中
tr = etree.parse('存储文件.html')
# r=tr.xpath('/html/body/div')
r=tr.xpath('/html//div/text()')[1]
# 后面的【n】 表示是第几个
print(r)
```

python 数字类型转换函数

函 数  作 用

int(x)  将 x 转换成整数类型

float(x)  将 x 转换成浮点数类型

complex(real，[,imag])  创建一个复数

str(x)  将 x 转换为字符串

repr(x)  将 x 转换为表达式字符串

eval(str)  计算在字符串中的有效 Python 表达式，并返回一个对象

chr(x)  将整数 x 转换为一个字符

ord(x)  将一个字符 x 转换为它对应的整数值

hex(x)  将一个整数 x 转换为一个十六进制字符串

oct(x)  将一个整数 x 转换为一个八进制的字符串

```python
import requests
from lxml import etree
import os
#导入 os 用于创建目录或者文件
#需求，爬取图片
# # 进行UA 伪装
headers={
    'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.0 Safari/537.36 Edg/103.0.1264.2'
}
url='http://pic.netbian.com/4kmeinv/'
# 获取数据
reponse=requests.get(url=url,headers=headers)
# 这里可以手动设置响应数据的编码，这解决不了可以使用另外一种方法：
reponse.encoding='utf-8' # 解决下面中文名乱码（不是万能的）(这里是对全部数据)
page_text=reponse.text
# 解析数据
# 解析 src 的属性值 和 alt 属性
tree = etree.HTML(page_text)
li_list=tree.xpath('//div[@class="slist"]/ul/li')
index=0
# 创建文件夹，存储
if not  os.path.exists('./picLibs'): # 如果不存在就创建一个
    os.mkdir('./pacLibs')
#这里有个注意点：
'''
mode 是权限
os.mkdir(path,mode=511) 创建一个目录
还有一个方法：
os.makedirs(name,mode=511,exist_ok=False)
这个是递归的创建目录，像 linux 的 mkdir -p 
os.rmdir(path) 删除指定的目录，目录非空就抛出 OSError 异常
'''
for i in li_list:
    #拼接域名使其完整
    img_src='http://pic.netbian.com'+i.xpath('./a/img/@src')[0]
    # img_name=i.xpath('./a/img/@alt')[0]+'.jpg'
    #方法二：通用处理中文乱码的解决办法（对部分数据）
    # img_name=img_name.encode('iso8859-1').decode('gbk')
    #这个方法在我这里时候失效了，并没有解决，反而报错
# UnicodeEncodeError: 'latin-1'
# codec
# can
# 't encode characters in position 0-2: ordinal not in range(256)
# 中文名乱码
    # 这里先用数字当作文件名
    index=index+1
    img_name=str(index)+'.jpg'
    # print(img_src,img_name)
    # 请求图片进行持久化存储 (是二进制存储的数据）
    img_data=requests.get(url=img_src,headers=headers).content
    img_path='pacLibs/'+img_name  # 图片路径
    with open(img_path,'wb') as fp:
        fp.write(img_data)
        print(img_name,'下载成功！！！')
```

```python
# 需求：解析出所以城市名称
from lxml import etree
import requests

url='https://www.aqistudy.cn/historydata/'
headers={
    'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.0 Safari/537.36 Edg/103.0.1264.2'
}

page_text=requests.get(url=url,headers=headers).text
tree=etree.HTML(page_text)
#===============================================================
# hot_li_list= tree.xpath('//div[@class="bottom"]/ul/li')
# all_city_names=[];
# # 解析到了热门城市
# for li in hot_li_list:
#     host_city_name=li.xpath('./a/text()')[0]
#     all_city_names.append(host_city_name)
#
# all_city_names.append("=============")
# # 解析全部城市的名称
# city_names=tree.xpath('//div[@class="bottom"]/ul/div[2]/li')
# for li in city_names:
#     a_city=li.xpath('./a/text()')[0]
#     all_city_names.append(a_city)
# print(all_city_names)
# ================================
# 对上面的进行改进
# 把上面分两步进行的综合为一步
# div/ul/li/a   热门城市a标签的层级关系
# div/ul/div[2]/li/a  全部城市a标签的层次关系
# 用 | 运算符可以表示两个同时可以作用，要么是这个 ，要么是那个
q_list=tree.xpath('//div[@class="bottom"]/ul/li/a | //div[@class="bottom"/ul/div[2]/li/a')
# 我这里实验出问题：报错像这样
all_city_name=[]
for a in  q_list:
    city_name=a.xpath('./text()')[0]
    all_city_name.append(city_name)
print(all_city_name,len(all_city_name))
```

# 验证码

[http://www.fateadm.com/price.html](http://www.fateadm.com/price.html)

![](image/image_QKdKZeISxa.png#alt=)

一般都是收费的

[https://www.jfbym.com/price/](https://www.jfbym.com/price/)

这个平台不错

[https://www.jfbym.com/own/index/index.html](https://www.jfbym.com/own/index/index.html)

我的个人中心

[https://console.bce.baidu.com/#/index/overview](https://console.bce.baidu.com/#/index/overview)

最终我是使用 百度的来实现的，每个月都有免费的次数

验证码：是一个反爬机制  

需要验证码图片中的数据，用于模拟登陆操作  

操作：  

- 
人工肉眼识别（不想使用）  

- 
第三方自动识别（非常不错）  

   - 云打码  

[http://www.yundama.com](http://www.yundama.com) (无法使用了 )[http://www.fateadm.com/](http://www.fateadm.com/) 这个可以  

[https://www.jfbym.com/price/](https://www.jfbym.com/price/) 这个也可以，这两个平台我都注册了  

都需要积分  

```python
# encoding:utf-8  

'''  

通用文字识别（高精度版）  

使用命令： pip install baidu-aip 
'''  

# 请求格式： POST 方式调用  

from aip import AipOcr  

#[https://console.bce.baidu.com/ai/#/ai/ocr/app/detail~appId=3311072](https://console.bce.baidu.com/ai/#/ai/ocr/app/detail~appId=3311072)  

#这个参数需要创建应用，创建应用后可以查看  

# 创建应用都需要这个步骤  

# 你的 APPID AK SK
APP_ID = '26430357'  

API_KEY = 'bCzk4yDAfB4Ewx19c9W0iPEh'  

SECRET_KEY = 'LKI88PDoLGFLOr0AZ1YsMTQDcYGjKoqd'  

client = AipOcr(APP_ID, API_KEY, SECRET_KEY)  

# 读取图片  

def get_file_content(filePath):  

  with open(filePath, 'rb') as fp:  

  return fp.read()  

image = get_file_content('./img.png')  

# 调用通用文字识别, 图片参数为本地图片  

result = client.basicGeneral(image)  

# 定义参数变量  

options = {  

  # 定义图像方向  

  'detect_direction' : 'true',  

    # 识别语言类型，默认为'CHN_ENG'中英文混合  

  'language_type' : 'CHN_ENG',  

}  

# 调用通用文字识别接口  

result = client.basicGeneral(image,options)  

print(result)  

#Open api qps request limit reached 报这个错误说明没有次数了  

# [https://console.bce.baidu.com/ai/#/ai/ocr/overview/resource/getFree](https://console.bce.baidu.com/ai/#/ai/ocr/overview/resource/getFree)  

# 上面这个网站每月可以领取免费额度  

# 识别结果：{'direction': 0, 'words_result': [{'words': 'P4RK'}],  

# 'words_result_num': 1, 'log_id': 1535870016417778508}
```

[https://console.bce.baidu.com/ai/#/ai/ocr/overview/resource/getFree](https://console.bce.baidu.com/ai/#/ai/ocr/overview/resource/getFree)

上面这个网站每月可以领取免费额度

实现思路：

把验证码图片，爬取下来，然后调用识别，再从结果中提取到验证码，再填写

- 
把图片下载到本地

- 
调用接口进行识别


模拟登录：  

- 
网站点击登录后会发起一个 POST 请求  

- 
POST 请求中会携带登录之前录入的相关的登录信息（用户名、密码、验证码...）


# 这个 xpath 不用自己写，浏览器打开开发者工具，点击元素，直接右击复制需要的 xpath 

![](image/image_tgM3_M7zsL.png#alt=)

```python
# 模拟登录
'''
编码流程：
1. 验证码的识别，获取验证码
2. 对 POST 请求进行发送（处理请求参数）
3. 对响应的数据进行持久化存储
'''
import requests
from aip import AipOcr
from lxml import etree
headers={
    'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.0 Safari/537.36 Edg/103.0.1264.2'
}
url='https://so.gushiwen.cn/user/login.aspx?from=http://so.gushiwen.cn/user/collect.aspx'
page_text=requests.get(url=url,headers=headers).text
tree = etree.HTML(page_text)
# //*[@id="imgCode"]
code_img_src=tree.xpath('//*[@id="imgCode"]/@src')[0]
print(code_img_src)
code_img_data=requests.get(url=code_img_src,headers=headers).content
with open('./img.png','wb') as fp:
    fp.write(code_img_data)
# 使用百度云进行识别
APP_ID = '26430357'
API_KEY = 'bCzk4yDAfB4Ewx19c9W0iPEh'
SECRET_KEY = 'LKI88PDoLGFLOr0AZ1YsMTQDcYGjKoqd'

client = AipOcr(APP_ID, API_KEY, SECRET_KEY)

# 读取图片
def get_file_content(filePath):
    with open(filePath, 'rb') as fp:
        return fp.read()

image = get_file_content('./img.png')

#  调用通用文字识别, 图片参数为本地图片
result = client.basicGeneral(image)


# 定义参数变量
options = {
    # 定义图像方向
        'detect_direction' : 'true',
    # 识别语言类型，默认为'CHN_ENG'中英文混合
        'language_type' : 'CHN_ENG',
}
# 调用通用文字识别接口
result = client.basicGeneral(image,options)
print(result)
# 我上面这个验证码图片有问题无法下载下来
# 数据准备好以后，使用 Post 方式发起请求，把数据传送进去，url 给成登录页面的 url
# 用户名和密码是固定不变的，注意密码一般是加密的，需要用抓包工具查看。
# 包括也可以查看 post 请求所需要携带的参数
data={
    'email':'',
    'key_id':'',
    '...':'...'
}
requests.post(url=url,headers=headers,data=data)
```

# 把自己的百度 OCR 封装成一个函数  

```python
def baidu_self_ocr(filePath):  

#==============================这里是我的固定数据  

  APP_ID = '26430357'  

  API_KEY = 'bCzk4yDAfB4Ewx19c9W0iPEh'  

  SECRET_KEY = 'LKI88PDoLGFLOr0AZ1YsMTQDcYGjKoqd'  

  client = AipOcr(APP_ID, API_KEY, SECRET_KEY)  

    # 定义参数变量  

  options = {  

    # 定义图像方向  

  'detect_direction' : 'true',  

    # 识别语言类型，默认为'CHN_ENG'中英文混合  

  'language_type' : 'CHN_ENG',  

    }  

    with open(filePath, 'rb') as fp:  

  image=fp.read()  

    return client.basicGeneral(image,options)  

# 使用  

print(baidu_self_ocr('./img.png'))
```

注意使用需要导包：`from aip import AipOcr`

#### 代理

> 破解封 IP 这种反爬机制


代理服务器。

代理的作用：

- 
突破自身 IP 访问限制

- 
可以隐藏我们真实的 IP 


相关网站：

- 
快代理

[https://www.kuaidaili.com/](https://www.kuaidaili.com/)


```python
# 代理
# 代理 ip 的类型：
'''
- http:只能用于 http 的请求
- https ： 只能用于 https 的请求
代理 ip 的匿名度：
透明：服务器知道这次请求使用了代理，也知道请求对应的真实IP
匿名：知道使用了代理，不知道真实 IP
高匿：不知道使用了代理，更不知道真实 ip

'''
import requests

headers={
    'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.0 Safari/537.36 Edg/103.0.1264.2'
}
url='https://www.baidu.com/s?wd=ip'

page_text=requests.get(url=url,headers=headers,proxies={"https":'127.0.0.1'}).text
with open('ip.html','w',encoding='utf-8') as fp:
    fp.write(page_text)
# 就是使用 proxies 指定代理
```

# 异步爬虫

> 用异步实现高性能的数据爬取操作


```python
# ============异步爬虫===============================
import requests
headers={
    'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.0 Safari/537.36 Edg/103.0.1264.2'
}
urls =[
    'http://bizhi360.com/dongman/',
     'https://xinzhuobu.com/?cat=1&order=hot',
    'https://4k-m.com/b/?l=%e5%8a%a8%e6%bc%ab']

def get_contest(url):
    print("正在爬取:",url)
    # get 方法是一个阻塞方法
    response=requests.get(url=url,headers=headers)
    if response.status_code == 200 : # 请求成功
        return response.content
    else:
        print("请求失败!!!")

# 用于数据解析
def parse_context(content):
    print("响应数据的长度为",len(content))

for url in urls:
    content=get_contest(url)
    parse_context(content)
```

方式：

- 
多线程、多进程（不建议）

   - 
好处：可以异步执行

   - 
弊端：不可以无限制的开启

- 
线程池、进程池（适当使用）

   - 
好处：可以降低系统对进程或线程创建和销毁的频率，从而提升效率

   - 
弊端：池中线程或进程的数量是有上限的


测试模拟：

```python
import time

def get_page(str):
    print("down...")
    time.sleep(2)
    print("success..")
name_list=['s','y','ss','yy']

start_time=time.time()
for i in range(len(name_list)):
    get_page(name_list[i])
end_time=time.time()
print("花费时间为 ",(end_time-start_time))
```

引入线程池：

```python
#==============线程池的使用=============================
import time
# 引入线程池模块
from multiprocessing.dummy import Pool
# 使用线程池执行
start_time=time.time()
def get_page(str):
    print("down...")
    time.sleep(2)
    print("success..")
name_list=['s','y','ss','yy']

# 实例化一个线程池对象
# 第一个参数表示线程池中线程的对象
pool=Pool(1)
# 将 列表中每一个元素传递给 get_page 进行处理
# 方法有返回值，就用列表去接收，没有就可以不用考虑
pool.map(get_page,name_list)
end_time=time.time()
print("花费时间为 ",(end_time-start_time))
pool.join()
pool.close()
```

**用完线程池需要关闭下，再等待所有子线程全部执行完再结束。** 

如果遇到 bs4 和 xpath 不能解析的数据时，使用正则表达式解析。要灵活使用

# 单线程+异步协方式：

相关概念：  

- 
event_loop 事件循环，相当于一个无限循环，可以把一些函数注册到这个事件循环上，当满足某些条件的时候，函数就会被循环执行  

- 
coroutine 协程对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用，可以合使用 async 关键字来定义一个方法，这个方法在调用时不会立即执行，而是返回一个协程对象  

- 
task 任务，它是对协程对象的进一步封装，包含了任务的各个状态  

- 
future 代表将来执行或还没有执行的任务，实际上和 task 没有本质区别  

- 
async 定义一个协程  

- 
await 用来挂起阻塞方法的执行


```python
import asyncio
async def request(url): # async 修饰函数
    print("正在请求的 url 是：",url)
    print("请求成功：",url)
    return  url
c=request("www.baidu.com") #async 修饰的函数， 返回一个协程对象

# 创建一个事件循环对象
# loop = asyncio.get_event_loop()
#
# # 将协程对象注册到 loop 中，然后可以启动 loop
# loop.run_until_complete(c)  # 方法到这里才会真正执行

# task 使用
# loop = asyncio.get_event_loop();
# # 基于 loop 创建了一个 task 对象
# task = loop.create_task(c)
# print(task)
# loop.run_until_complete(task)
# print(task)
# task 的 pending 说明任务没有执行，finished 说明任务已经执行完成
# task 输出：<Task finished name='Task-1' coro=<request() done, defined at /home/syhk/sywork/work/vscodework/VScodeProjects/Pythonprojects/爬虫/demo.py:472> result=None>
# future 的使用
# loop=asyncio.get_event_loop()
# future=loop.create_future()
# print(future)
# loop.run_until_complete(future)
# print(future)
# future 输出：<Future pending>

def callback_func(task):
    # result 返回的就是任务对象中封装的协程对象对应函数的返回值
    print(task.result())


# 回调绑定
loop=asyncio.get_event_loop()
task=asyncio.ensure_future(c)
#将回调函数绑定到任务对象中
task.add_done_callback(callback_func)
loop.run_until_complete(task)
```

```python
import asyncio
import time


async def request(url):
    print("正在下载", url)
    # 在异步协程中如果出现了同步模块相关的代码，那么就无法实现异步。
    # time.sleep(2)
    # 当要 asyncio 中遇到阻塞操作必须进行手动挂起
    await asyncio.sleep(2)
    print('下载完成', url)


start = time.time()
urls = [
    'www.baidu.com',
    'www.sogou.com',
    'www.goubanjia.com'
]
# 任务列表:存放多个任务对象
stasks= []
for url in urls:
    c = request(url)
    task = asyncio.ensure_future(c)
    stasks.append(task)
loop = asyncio.get_event_loop()
# 固定语法格式，需要将任务列表封装到 wait 中
loop.run_until_complete(asyncio.wait(stasks))
print(time.time() - start)
```

# aiohttp ：基于异步网络请求的模块

`pip install aiohttp` 下载模块

```python
import asyncio
import time
# 环境的安装
# pip install aiohttp
import aiohttp

start = time.time()
urls = [
    'https://www.baidu.com',
    'https://www.sogou.com',
    'https://www.taobao.com'
]


# 发起基于异步的网络请求
async def get_page(url):
    async with aiohttp.ClientSession() as session:
        # get() 、post() 
        #headers (url,headers=headers)，请求携带的参数和 request 一样，
        # proxy 使用代理 proxy='https://ip:port'
        async with await session.get(url) as response:
            # text() 方法可以返回字符串形式的响应数据
            # read() 返回的是二进制的响应数据
            # json () 返回的是 json 对象
            print("run", url)
            page_text = await response.text()
            print(page_text)
            print("完成...")


tasks = []

for url in urls:
    c = get_page(url)
    task = asyncio.ensure_future(c)
    tasks.append(task)
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
print(time.time() - start)
```

### selenium 模块的基本使用

1. 
便捷的获取网站中动态加载的数据  

2. 
便捷实现模拟登录  


基于浏览器自动化的模块  

# selenium 使用流程  

- 
环境安装： pip install selenium  

- 
下载一个浏览器的驱动程序  

   - 
下载路径：[https://msedgewebdriverstorage.z22.web.core.windows.net/](https://msedgewebdriverstorage.z22.web.core.windows.net/)  (edge的)  

[https://liushilive.github.io/github_selenium_drivers/md/Selenium.html](https://liushilive.github.io/github_selenium_drivers/md/Selenium.html)  

[https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/](https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/)  

（上面这个网站比较齐全）  


# selenium 的作用：  

   1. 
便捷的获取网站中动态加载的数据  

   2. 
便捷实现模拟登录  


# selenium 使用流程  

   - 
环境安装： pip install selenium  

   - 
下载一个浏览器的驱动程序  

      - 
下载路径：[https://msedgewebdriverstorage.z22.web.core.windows.net/](https://msedgewebdriverstorage.z22.web.core.windows.net/)  (edge的)  

[https://liushilive.github.io/github_selenium_drivers/md/Selenium.html](https://liushilive.github.io/github_selenium_drivers/md/Selenium.html)  

[https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/](https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/)  

（上面这个网站比较齐全）  


      - 
实例化一个浏览器对象  

      - 
编写基于浏览器自动化操作的代码  

      - 
发起请求：get(url)  

         - 标签定位：find 系列的方法  
      - 
标签交互：send_keys('xxx')  

         - 
执行js 程序：excute_srcipt('js代码'')  

         - 
前进、后退：back(),forward()  

         - 
关闭:quit()  

         - 
selenium 处理 iframe  

            - 
如果定位的标签存在于 iframe 标签之中，则必须使用 switch_to.frame(id)  

            - 
动作链（拖动操作）： from selenium.webdriver import ActionChains  

               - 
实例化一个动作链对象: action=ActionChains(bro)  

               - 
click_and_hold ：长按且点击操作  

      - 
move_by_offset(x,y)   

         - perform() 让动作链立即执行  
      - 
释放动作链对象release()


![](image/image_rsohl1kQre.png#alt=)
   - 
实例化一个浏览器对象  

   - 
编写基于浏览器自动化操作的代码


```python
from  selenium import webdriver

# 实例化一个浏览器对象，一定要传入浏览器的驱动对象，也就是路径
bro = webdriver.Chrome(executable_path='./chromedriver')
bro.get('https://www.baidu.com/') # 打开后会请求这个地址
```

这里注意一个问题：

我把 Chrome 的c 写成了小写，就一直报错

```python
# 这个是就是要查看网页源代码，需要通过什么方式去定位，要执行操作的元素
from  selenium import webdriver
from  time import  sleep

# 实例化一个浏览器对象，一定要传入浏览器的驱动对象，也就是路径
bro = webdriver.Chrome(executable_path='./chromedriver')

bro.get('https://taobao.com/')

# 标签定位
serch_input=bro.find_element_by_id('q')
# 标签交互
serch_input.send_keys('Iphone')

# selector 选择器
btn = bro.find_element_by_css_selector('.btn-search')
btn.click() # 点击搜索按钮，也就是上面通过选择器选择到的元素

sleep(5)
# 退出浏览器
bro.quit()
```

 改进 ：

```python
# 这个是就是要查看网页源代码，需要通过什么方式去定位，要执行操作的元素
from  selenium import webdriver
from  time import  sleep

# 实例化一个浏览器对象，一定要传入浏览器的驱动对象，也就是路径
bro = webdriver.Chrome(executable_path='./chromedriver')

bro.get('https://taobao.com/')

# 标签定位
serch_input=bro.find_element_by_id('q')
# 标签交互
serch_input.send_keys('Iphone')

# 执行一组 js 程序:执行js 代码实现滚屏翻页
bro.execute_script('window.scrollTo(0,document.body.scrollHeight)')
sleep(2)

# selector 选择器
btn = bro.find_element_by_css_selector('.btn-search')
btn.click() # 点击搜索按钮，也就是上面通过选择器选择到的元素

bro.get('https://www.baidu.com/')
sleep(2)

# 回退的意思;就是点击返回按钮
bro.back()

# 前进
bro.forward()
sleep(5)

# 退出浏览器
bro.quit()
```

```python
from  selenium import webdriver
# 导入动作链
from selenium.webdriver import ActionChains
from time import sleep
bro = webdriver.Chrome(executable_path='./chromedriver')
bro.get('https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable')

# 如果定位的标签是存在于  iframe 标签之中的则必须通过如下操作进行标签定位
bro.switch_to.frame('iframeResult')  # 切换浏览器标签定位的作用域
div=bro.find_element_by_id('draggable')

# 动作链
action=ActionChains(bro)

action.click_and_hold(div)

for i in range(7):
    # perform 表示立即执行操作链
    #使用这个方法要注意要传入两个参数 （x,y） x 表示水平方向 ,y 表示竖直方向
    action.move_by_offset(17,0).perform()
    sleep(0.3)

# 释放动作链
action.release()

bro.quit()
```

模拟QQ空间登录

```python
# 模拟 qq 空间登录
from selenium import webdriver
from time import  sleep

bro=webdriver.Chrome(executable_path='./chromedriver')

bro.get('https://qzone.qq.com/')

bro.switch_to.frame('login_frame')

a_tag=bro.find_element_by_id("switcher_plogin")

a_tag.click()

userName_tag = bro.find_element_by_id('u')
password_tag = bro.find_element_by_id('p')

sleep(1)
userName_tag.send_keys('725482520')
password_tag.send_keys('20201028')
sleep(1)
btn=bro.find_element_by_id('login_button')
btn.click()

sleep(3)

bro.quit()
```

### 无头浏览器

```python
# 无可视化界面 （无头浏览器）
# 就是让浏览器后台执行，不弹出窗口
from selenium import  webdriver
# 实现无可视化界面
from selenium.webdriver.chrome.options import Options
from  time import sleep
# 实现规避检测
from selenium.webdriver import ChromeOptions


# 实现无可视界面的操作
chrom_options=Options()
chrom_options.add_argument('--headless')
chrom_options.add_argument('--disable-gpu')



# 无头浏览器 也不会影响使用当前使用的浏览器  (phantomJs)
# 如何实现让 selenium 规避被检测到的风险
option=ChromeOptions()
option.add_experimental_option('excludeSwitches',['enale-automation'])


bro=webdriver.Chrome(executable_path='./chromedriver',chrome_options=chrom_options,options=option)

bro.get('https://ww.baidu.com')
print(bro.page_source)
sleep(2)
bro.quit()
```

#### 超级鹰

[https://www.chaojiying.com/](https://www.chaojiying.com/)

超级鹰 （解决图片验证码，就像给一些图片选择符合要求的图片）

# scrapy 框架

爬虫中封装好的一个明星框架。功能：高性能的持久化存储，异步的数据下载，高性能的数据分析，分布式

- 
安装： pip install scrapy  

   - 上面直接命令安装是 mac or linux  
- 
创建一个工程: scrapy startproject xxxPro

- 
在 spiders 子目录中创建一个爬虫文件  

   - scrapy genspider 文件名称 [www.xxx.com](http://www.xxx.com)
- 
执行工程： scrapy crawl 文件名称（就是上面创建的）


目录结构：

![](image/image_jw5d_LsUIk.png#alt=)

配置文件中：

```python
# Obey robots.txt rules
ROBOTSTXT_OBEY = True
```

就是遵循 robots.txt 协议，一般设置为 false ，否则几乎没什么网站可以爬取。
